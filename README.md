# Efficient Token Transfer Learning for Cross-Style Music Generation
Equally Contributed by: Hanxi Xiao, Aaron Rosen.
Music, like words, has inherent semantics and meaning. The composition, order of the notes, are tied to the expressiveness of the music piece such as genre, tempo, instrumentation and more. Understanding and replicating semantics are crucial for generating music that is coherent and emotionally resonant. A principal obstacle in music generation is the considerable variation in the Markovian relationships among notes, which significantly fluctuates based on the music's style. This variation include factors such as genre, the number of soundtracks (ranging from solo instruments to full symphonies), and the historical context or era in which the music was produced. Transformers, with their attention mechanisms, are excellent at capturing dependencies and relationships in sequential data, making them ideal for understanding music semantics. However, transformers training often requires high computational resources. Here, we aim to leverage large pre-trained transformer and incorporate markovian models to generalize the pre-tranied transformer to unseen music style/genres for new music generation. Markov models can capture the transition probabilities between musical notes, enabling the generation of musically coherent sequences. We present a music generation model leveraging different neural network architectures to extract informative embedding and employ Markov models to comprehend and replicate the semantics of music, facilitating the creation of coherent and emotionally resonant musical compositions.


## Methods
The following observations motivate our project aims: audio files in MIDI format provide a parseable, rich language for modeling music; transformers can adequately learn contextual relationships in a large corpus of MIDI data, but are expensive to train and run; and probabilistic models like GCNs can be lightweight and efficient methods for leveraging these attention-based relationships for music generation. We seek to utilize embeddings and standard tokenizer from \href{https://github.com/asigalov61/Full-MIDI-Music-Transformer/tree/main#project-los-angeles}{Project Los Angeles' Full MIDI Music Transformer}, which was trained on the tokenized high quality \href{https://colinraffel.com/projects/lmd/}{Lakh MIDI (full) dataset} composed over 176,581 unique MIDI files (Fig.\ref{fig:midi}). 

We aim to leverage the advanced capabilities of a cutting-edge transformer for the rapid and efficient generation of cross-style music. Our goal is to create musical sequences in styles that are absent from the transformer's training data (Fig. \ref{fig:token_distribution}). To achieve this, we utilize the transformer's token and positional embeddings. Furthermore, we construct a neighboring adjacency matrix with data from styles not present in the Lakh dataset. This approach effectively captures the sequential information of the tokens. In our methodology, we integrate the transformer's refined embeddings with the sequential information from the unseen dataset, channeling this amalgamated input into a GCN. This strategy enables us to develop embeddings that are not only informed by the transformer's accurate embeddings but also by the genre- or style-specific data extracted from the neighboring adjacency matrix. Subsequently, we implement a biased RWR algorithm, utilizing the GCN node embeddings to generate innovative musical sequences. This technique ensures that our newly composed pieces are firmly rooted in the learned embeddings while also being distinctly influenced by the stylistic elements of the previously unseen data(Fig.\ref{fig:model}).


Model steps are 
1. Tokenize new midi files (MIDI files available at https://github.com/asigalov61/Tegridy-MIDI-Dataset)
2. Use gpu_extract_embeddings_from_forward.ipynb to get positional and token embeddings
3. Use make_new_token_adj.ipynb to make adjacency matrix using token neighbors in new tokenized data
4. Run GCN.py to train GCN using transformer embeddings and RWR to generate new sequences
